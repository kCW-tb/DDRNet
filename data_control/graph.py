import matplotlib.pyplot as plt
import pandas as pd
import os
import io

def plot_and_save_metrics_from_log(save_dir="training_plots"):
    """
    주어진 학습 로그 데이터를 사용하여 손실 및 학습률 그래프를 생성하고 파일로 저장합니다.
    """
    # --- 제공해주신 전체 로그 데이터를 문자열로 저장 ---
    # Epoch, Train-loss, Val-loss, learningRate 순서
    log_data = """
        Epoch		Train-loss		Val-loss		learningRate

        1		5.9139		N/A		0.00002004
        2		3.6741		N/A		0.00004003
        3		2.6756		N/A		0.00006002
        4		2.3517		N/A		0.00008001
        5		2.1469		1.3198		0.00010000
        6		1.9777		N/A		0.00009969
        7		1.7760		N/A		0.00009939
        8		1.6182		N/A		0.00009908
        9		1.5070		N/A		0.00009878
        10		1.4134		0.8850		0.00009847
        11		1.3493		N/A		0.00009817
        12		1.2931		N/A		0.00009786
        13		1.2440		N/A		0.00009756
        14		1.1871		N/A		0.00009725
        15		1.1532		0.6542		0.00009694
        16		1.1240		N/A		0.00009664
        17		1.0961		N/A		0.00009633
        18		1.0627		N/A		0.00009603
        19		1.0366		N/A		0.00009572
        20		1.0144		0.5590		0.00009541
        21		0.9957		N/A		0.00009511
        22		0.9820		N/A		0.00009480
        23		0.9461		N/A		0.00009449
        24		0.9363		N/A		0.00009418
        25		0.9161		0.5226		0.00009388
        26		0.9116		N/A		0.00009357
        27		0.8932		N/A		0.00009326
        28		0.8987		N/A		0.00009295
        29		0.8770		N/A		0.00009265
        30		0.8670		0.4612		0.00009234
        31		0.8523		N/A		0.00009203
        32		0.8240		N/A		0.00009172
        33		0.8424		N/A		0.00009142
        34		0.8123		N/A		0.00009111
        35		0.8165		0.4332		0.00009080
        36		0.8053		N/A		0.00009049
        37		0.7938		N/A		0.00009018
        38		0.7983		N/A		0.00008987
        39		0.7834		N/A		0.00008956
        40		0.7794		0.4151		0.00008926
        41		0.7606		N/A		0.00008895
        42		0.7613		N/A		0.00008864
        43		0.7576		N/A		0.00008833
        44		0.7568		N/A		0.00008802
        45		0.7411		0.3871		0.00008771
        46		0.7402		N/A		0.00008740
        47		0.7284		N/A		0.00008709
        48		0.7217		N/A		0.00008678
        49		0.7170		N/A		0.00008647
        50		0.7177		0.3749		0.00008616
        51		0.7151		N/A		0.00008585
        52		0.7064		N/A		0.00008554
        53		0.7086		N/A		0.00008523
        54		0.6973		N/A		0.00008492
        55		0.6923		0.3723		0.00008461
        56		0.6972		N/A		0.00008430
        57		0.6805		N/A		0.00008399
        58		0.6824		N/A		0.00008367
        59		0.6743		N/A		0.00008336
        60		0.6706		0.3560		0.00008305
        61		0.6689		N/A		0.00008274
        62		0.6678		N/A		0.00008243
        63		0.6609		N/A		0.00008212
        64		0.6581		N/A		0.00008181
        65		0.6631		0.3545		0.00008149
        66		0.6575		N/A		0.00008118
        67		0.6541		N/A		0.00008087
        68		0.6490		N/A		0.00008056
        69		0.6398		N/A		0.00008024
        70		0.6263		0.3490		0.00007993
        71		0.6354		N/A		0.00007962
        72		0.6427		N/A		0.00007931
        73		0.6282		N/A		0.00007899
        74		0.6402		N/A		0.00007868
        75		0.6276		0.3206		0.00007837
        76		0.6347		N/A		0.00007805
        77		0.6224		N/A		0.00007774
        78		0.6125		N/A		0.00007742
        79		0.6163		N/A		0.00007711
        80		0.6109		0.3301		0.00007680
        81		0.6135		N/A		0.00007648
        82		0.6068		N/A		0.00007617
        83		0.5898		N/A		0.00007585
        84		0.6029		N/A		0.00007554
        85		0.5941		0.3238		0.00007522
        86		0.5955		N/A		0.00007491
        87		0.5923		N/A		0.00007459
        88		0.5867		N/A		0.00007428
        89		0.5880		N/A		0.00007396
        90		0.5872		0.3281		0.00007365
        91		0.5850		N/A		0.00007333
        92		0.5824		N/A		0.00007302
        93		0.5891		N/A		0.00007270
        94		0.5747		N/A		0.00007238
        95		0.5742		0.3097		0.00007207
        96		0.5804		N/A		0.00007175
        97		0.5779		N/A		0.00007143
        98		0.5790		N/A		0.00007112
        99		0.5727		N/A		0.00007080
        100		0.5763		0.3092		0.00007048
        101		0.5731		N/A		0.00007017
        102		0.5590		N/A		0.00006985
        103		0.5618		N/A		0.00006953
        104		0.5620		N/A		0.00006921
        105		0.5632		0.2991		0.00006890
        106		0.5586		N/A		0.00006858
        107		0.5580		N/A		0.00006826
        108		0.5649		N/A		0.00006794
        109		0.5437		N/A		0.00006762
        110		0.5570		0.2852		0.00006730
        111		0.5546		N/A		0.00006698
        112		0.5519		N/A		0.00006667
        113		0.5423		N/A		0.00006635
        114		0.5472		N/A		0.00006603
        115		0.5422		0.2918		0.00006571
        116		0.5349		N/A		0.00006539
        117		0.5398		N/A		0.00006507
        118		0.5365		N/A		0.00006475
        119		0.5369		N/A		0.00006443
        120		0.5401		0.2872		0.00006411
        121		0.5389		N/A		0.00006379
        122		0.5327		N/A		0.00006347
        123		0.5364		N/A		0.00006314
        124		0.5406		N/A		0.00006282
        125		0.5317		0.3029		0.00006250
        126		0.5258		N/A		0.00006218
        127		0.5275		N/A		0.00006186
        128		0.5244		N/A		0.00006154
        129		0.5295		N/A		0.00006121
        130		0.5230		0.2754		0.00006089
        131		0.5266		N/A		0.00006057
        132		0.5205		N/A		0.00006025
        133		0.5207		N/A		0.00005992
        134		0.5190		N/A		0.00005960
        135		0.5131		0.2880		0.00005928
        136		0.5215		N/A		0.00005895
        137		0.5100		N/A		0.00005863
        138		0.5168		N/A		0.00005831
        139		0.5132		N/A		0.00005798
        140		0.5145		0.2836		0.00005766
        141		0.5140		N/A		0.00005733
        142		0.5019		N/A		0.00005701
        143		0.5159		N/A		0.00005669
        144		0.5072		N/A		0.00005636
        145		0.5045		0.2710		0.00005603
        146		0.5090		N/A		0.00005571
        147		0.5134		N/A		0.00005538
        148		0.5118		N/A		0.00005506
        149		0.5017		N/A		0.00005473
        150		0.5033		0.2683		0.00005441
        151		0.5008		N/A		0.00005408
        152		0.4966		N/A		0.00005375
        153		0.5047		N/A		0.00005343
        154		0.4894		N/A		0.00005310
        155		0.5040		0.2648		0.00005277
        156		0.4921		N/A		0.00005244
        157		0.4975		N/A		0.00005211
        158		0.4884		N/A		0.00005179
        159		0.4962		N/A		0.00005146
        160		0.4987		0.2712		0.00005113
        161		0.4977		N/A		0.00005080
        162		0.4910		N/A		0.00005047
        163		0.4937		N/A		0.00005014
        164		0.4919		N/A		0.00004981
        165		0.4896		0.2682		0.00004948
        166		0.4893		N/A		0.00004915
        167		0.4952		N/A		0.00004882
        168		0.4787		N/A		0.00004849
        169		0.4834		N/A		0.00004816
        170		0.4799		0.2689		0.00004783
        171		0.4846		N/A		0.00004750
        172		0.4839		N/A		0.00004717
        173		0.4781		N/A		0.00004684
        174		0.4775		N/A		0.00004650
        175		0.4760		0.2569		0.00004617
        176		0.4801		N/A		0.00004584
        177		0.4810		N/A		0.00004551
        178		0.4731		N/A		0.00004517
        179		0.4752		N/A		0.00004484
        180		0.4742		0.2604		0.00004451
        181		0.4744		N/A		0.00004417
        182		0.4804		N/A		0.00004384
        183		0.4707		N/A		0.00004350
        184		0.4730		N/A		0.00004317
        185		0.4769		0.2596		0.00004283
        186		0.4722		N/A		0.00004250
        187		0.4751		N/A		0.00004216
        188		0.4664		N/A		0.00004183
        189		0.4701		N/A		0.00004149
        190		0.4684		0.2645		0.00004115
        191		0.4700		N/A		0.00004082
        192		0.4686		N/A		0.00004048
        193		0.4639		N/A		0.00004014
        194		0.4657		N/A		0.00003980
        195		0.4623		0.2538		0.00003947
        196		0.4653		N/A		0.00003913
        197		0.4575		N/A		0.00003879
        198		0.4619		N/A		0.00003845
        199		0.4622		N/A		0.00003811
        200		0.4594		0.2643		0.00003777
        201		0.4600		N/A		0.00003743
        202		0.4569		N/A		0.00003709
        203		0.4567		N/A		0.00003675
        204		0.4641		N/A		0.00003641
        205		0.4545		0.2510		0.00003607
        206		0.4541		N/A		0.00003573
        207		0.4568		N/A		0.00003538
        208		0.4558		N/A		0.00003504
        209		0.4576		N/A		0.00003470
        210		0.4544		0.2480		0.00003435
        211		0.4525		N/A		0.00003401
        212		0.4534		N/A		0.00003367
        213		0.4621		N/A		0.00003332
        214		0.4438		N/A		0.00003298
        215		0.4555		0.2533		0.00003263
        216		0.4518		N/A		0.00003229
        217		0.4532		N/A		0.00003194
        218		0.4467		N/A		0.00003159
        219		0.4458		N/A		0.00003125
        220		0.4471		0.2506		0.00003090
        221		0.4487		N/A		0.00003055
        222		0.4459		N/A		0.00003020
        223		0.4477		N/A		0.00002985
        224		0.4358		N/A		0.00002950
        225		0.4481		0.2493		0.00002916
        226		0.4412		N/A		0.00002881
        227		0.4446		N/A		0.00002845
        228		0.4463		N/A		0.00002810
        229		0.4428		N/A		0.00002775
        230		0.4451		0.2494		0.00002740
        231		0.4443		N/A		0.00002705
        232		0.4431		N/A		0.00002669
        233		0.4436		N/A		0.00002634
        234		0.4390		N/A		0.00002599
        235		0.4427		0.2434		0.00002563
        236		0.4424		N/A		0.00002528
        237		0.4404		N/A		0.00002492
        238		0.4371		N/A		0.00002456
        239		0.4395		N/A		0.00002421
        240		0.4408		0.2470		0.00002385
        241		0.4346		N/A		0.00002349
        242		0.4339		N/A		0.00002313
        243		0.4379		N/A		0.00002277
        244		0.4343		N/A		0.00002241
        245		0.4333		0.2392		0.00002205
        246		0.4321		N/A		0.00002169
        247		0.4345		N/A		0.00002133
        248		0.4322		N/A		0.00002097
        249		0.4301		N/A		0.00002061
        250		0.4319		0.2434		0.00002024
        251		0.4354		N/A		0.00001988
        252		0.4294		N/A		0.00001951
        253		0.4305		N/A		0.00001914
        254		0.4347		N/A		0.00001878
        255		0.4324		0.2362		0.00001841
        256		0.4333		N/A		0.00001804
        257		0.4351		N/A		0.00001767
        258		0.4285		N/A		0.00001730
        259		0.4353		N/A		0.00001693
        260		0.4307		0.2375		0.00001656
        261		0.4278		N/A		0.00001619
        262		0.4313		N/A		0.00001581
        263		0.4233		N/A		0.00001544
        264		0.4252		N/A		0.00001506
        265		0.4291		0.2395		0.00001468
        266		0.4281		N/A		0.00001431
        267		0.4251		N/A		0.00001393
        268		0.4250		N/A		0.00001355
        269		0.4247		N/A		0.00001316
        270		0.4239		0.2350		0.00001278
        271		0.4200		N/A		0.00001240
        272		0.4270		N/A		0.00001201
        273		0.4267		N/A		0.00001162
        274		0.4242		N/A		0.00001124
        275		0.4250		0.2458		0.00001085
        276		0.4223		N/A		0.00001046
        277		0.4182		N/A		0.00001006
        278		0.4239		N/A		0.00000967
        279		0.4187		N/A		0.00000927
        280		0.4206		0.2330		0.00000887
        281		0.4243		N/A		0.00000847
        282		0.4210		N/A		0.00000807
        283		0.4208		N/A		0.00000767
        284		0.4180		N/A		0.00000726
        285		0.4178		0.2297		0.00000685
        286		0.4193		N/A		0.00000644
        287		0.4178		N/A		0.00000602
        288		0.4174		N/A		0.00000560
        289		0.4151		N/A		0.00000518
        290		0.4156		0.2369		0.00000476
        291		0.4138		N/A		0.00000432
        292		0.4151		N/A		0.00000389
        293		0.4239		N/A		0.00000345
        294		0.4187		N/A		0.00000300
        295		0.4167		0.2392		0.00000255
        296		0.4190		N/A		0.00000208
        297		0.4206		N/A		0.00000161
        298		0.4170		N/A		0.00000112
        299		0.4175		N/A		0.00000060
        300		0.4137		0.2302		0.00000010
    """
    
    # ✍️ 수정된 부분: 첫 번째 줄은 헤더가 아니므로 skiprows=1을 제거하고,
    # pandas가 자동으로 헤더를 추론하도록 header=0을 사용합니다.
    # 또한, 열 이름을 직접 지정하여 파싱 오류를 방지합니다.
    df = pd.read_csv(io.StringIO(log_data), delim_whitespace=True, 
                      names=['Epoch', 'Train-loss', 'Val-loss', 'learningRate'], header=0)
    # df = pd.read_csv(io.StringIO(log_data), delim_whitespace=True, 
    #                  names=['Epoch', 'Train-loss', 'learningRate'], header=0)
    
    # Val-loss 열의 'N/A'를 숫자로 변환 (NaN으로 바뀜)
    df['Val-loss'] = pd.to_numeric(df['Val-loss'], errors='coerce')
    
    # 저장할 디렉터리가 없으면 생성
    os.makedirs(save_dir, exist_ok=True)

    # --- Loss 그래프 (Train vs Validation) ---
    plt.figure(figsize=(12, 7))
    plt.plot(df['Epoch'], df['Train-loss'], color='blue', marker='o', linestyle='-', markersize=4, label='Train Loss')
    
    # Val-loss에서 NaN이 아닌 유효한 데이터만 필터링하여 그립니다.
    val_df = df.dropna(subset=['Val-loss'])
    plt.plot(val_df['Epoch'], val_df['Val-loss'], color='green', marker='s', linestyle='--', markersize=5, label='Validation Loss')
    
    # 그래프 제목 및 라벨 설정
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss over Epochs')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    
    # 그래프를 파일로 저장
    loss_path = os.path.join(save_dir, 'loss_curves.png')
    plt.savefig(loss_path)
    plt.close() # 메모리 해제
    print(f"Loss 그래프가 '{loss_path}'에 저장되었습니다.")

    # --- learningRate 그래프 ---
    plt.figure(figsize=(12, 7))
    plt.plot(df['Epoch'], df['learningRate'], color='red', marker='x', linestyle='--')
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate over Epochs')
    plt.grid(True)
    plt.tight_layout()
    
    # 그래프를 파일로 저장
    lr_path = os.path.join(save_dir, 'learning_rate.png')
    plt.savefig(lr_path)
    plt.close() # 메모리 해제
    print(f"Learning rate 그래프가 '{lr_path}'에 저장되었습니다.")

# --- 스크립트 실행 시작점 ---
if __name__ == '__main__':
    plot_and_save_metrics_from_log()